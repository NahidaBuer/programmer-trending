import logging
from typing import Dict, Any
from datetime import datetime, timedelta

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.events import EVENT_JOB_EXECUTED, EVENT_JOB_ERROR

from ..core.config import get_settings
from ..services.crawl_service import crawl_service
from .summary_generator import summary_generator

logger = logging.getLogger(__name__)


class TaskScheduler:
    """ä»»åŠ¡è°ƒåº¦å™¨ï¼Œç®¡ç†å®šæ—¶çˆ¬å–ä»»åŠ¡"""
    
    def __init__(self):
        self.scheduler = AsyncIOScheduler()
        self.settings = get_settings()
        self._setup_event_listeners()
        
    def _setup_event_listeners(self) -> None:
        """è®¾ç½®äº‹ä»¶ç›‘å¬å™¨"""
        self.scheduler.add_listener(self._job_executed, EVENT_JOB_EXECUTED)
        self.scheduler.add_listener(self._job_error, EVENT_JOB_ERROR)
        
    def _job_executed(self, event) -> None:
        """ä»»åŠ¡æ‰§è¡ŒæˆåŠŸæ—¶çš„å›è°ƒ"""
        logger.info(f"Job {event.job_id} executed successfully at {datetime.now()}")
        
    def _job_error(self, event) -> None:
        """ä»»åŠ¡æ‰§è¡Œå¤±è´¥æ—¶çš„å›è°ƒ"""
        logger.error(f"Job {event.job_id} failed: {event.exception}")
        
    async def start(self) -> None:
        """å¯åŠ¨è°ƒåº¦å™¨"""
        try:
            # æ·»åŠ å®šæ—¶ä»»åŠ¡
            await self._add_crawl_jobs()
            
            # å¯åŠ¨è°ƒåº¦å™¨
            self.scheduler.start()
            logger.info("è°ƒåº¦å™¨å¯åŠ¨æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"å¯åŠ¨è°ƒåº¦å™¨å¤±è´¥: {e}")
            raise
    
    async def stop(self) -> None:
        """åœæ­¢è°ƒåº¦å™¨"""
        try:
            self.scheduler.shutdown(wait=True)
            logger.info("è°ƒåº¦å™¨åœæ­¢æˆåŠŸ")
        except Exception as e:
            logger.error(f"åœæ­¢è°ƒåº¦å™¨å¤±è´¥: {e}")
            
    async def _add_crawl_jobs(self) -> None:
        """æ·»åŠ çˆ¬å–ä»»åŠ¡"""
        crawl_interval = self.settings.crawl_interval_minutes
        added_jobs: list[str] = []
        
        # æ ¹æ®é…ç½®æ·»åŠ å®šæ—¶çˆ¬è™«ä»»åŠ¡
        if self.settings.enable_crawl_scheduler:
            self.scheduler.add_job(
                self.crawl_all_sources_job,
                trigger=IntervalTrigger(minutes=crawl_interval),
                id="crawl_all_sources",
                name="Crawl All Sources",
                max_instances=1,  # é˜²æ­¢é‡å¤æ‰§è¡Œ
                replace_existing=True,
                next_run_time=datetime.now() + timedelta(seconds=30),  # 30ç§’åå¼€å§‹é¦–æ¬¡æ‰§è¡Œ
            )
            added_jobs.append(f"çˆ¬å–ä»»åŠ¡ (é—´éš” {crawl_interval} åˆ†é’Ÿ)")
            logger.info(f"âœ… å·²å¯ç”¨å®šæ—¶çˆ¬è™«ä»»åŠ¡ï¼Œé—´éš” {crawl_interval} åˆ†é’Ÿ")
        else:
            logger.info("âŒ å®šæ—¶çˆ¬è™«ä»»åŠ¡å·²ç¦ç”¨ (ENABLE_CRAWL_SCHEDULER=false)")
        
        # æ ¹æ®é…ç½®æ·»åŠ å®šæ—¶æ‘˜è¦ç”Ÿæˆä»»åŠ¡
        if self.settings.enable_summary_scheduler:
            self.scheduler.add_job(
                self.generate_summaries_job,
                trigger=IntervalTrigger(minutes=crawl_interval),
                id="generate_summaries", 
                name="Generate AI Summaries",
                max_instances=1,  # é˜²æ­¢é‡å¤æ‰§è¡Œ
                replace_existing=True,
                next_run_time=datetime.now() + timedelta(minutes=2),  # 2åˆ†é’Ÿåå¼€å§‹é¦–æ¬¡æ‰§è¡Œ
            )
            added_jobs.append(f"æ‘˜è¦ç”Ÿæˆä»»åŠ¡ (é—´éš” {crawl_interval} åˆ†é’Ÿ)")
            logger.info(f"âœ… å·²å¯ç”¨å®šæ—¶AIæ‘˜è¦ä»»åŠ¡ï¼Œé—´éš” {crawl_interval} åˆ†é’Ÿ")
        else:
            logger.info("âŒ å®šæ—¶AIæ‘˜è¦ä»»åŠ¡å·²ç¦ç”¨ (ENABLE_SUMMARY_SCHEDULER=false)")
        
        # æ±‡æ€»ä¿¡æ¯
        if added_jobs:
            logger.info(f"ğŸ“… å·²æ·»åŠ å®šæ—¶ä»»åŠ¡: {', '.join(added_jobs)}")
        else:
            logger.warning("âš ï¸  æ‰€æœ‰å®šæ—¶ä»»åŠ¡éƒ½å·²ç¦ç”¨ï¼Œè°ƒåº¦å™¨å°†ç©ºè¿è¡Œ")
        
    async def crawl_all_sources_job(self) -> None:
        """çˆ¬å–æ‰€æœ‰æ•°æ®æºçš„å®šæ—¶ä»»åŠ¡"""
        try:
            logger.info("å¼€å§‹å®šæ—¶çˆ¬å–æ‰€æœ‰æ•°æ®æº")
            start_time = datetime.now()
            
            results = await crawl_service.crawl_all_sources(limit_per_source=30)
            
            # ç»Ÿè®¡ç»“æœ
            total_new_items = sum(len(items) for items in results.values())
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logger.info(
                f"çˆ¬å–å®Œæˆ: {total_new_items} ä¸ªæ–°æ–‡ç« æ¥è‡ª "
                f"{len(results)} ä¸ªæ•°æ®æºï¼Œè€—æ—¶ {duration:.2f} ç§’"
            )
            
            # è®°å½•å„æ•°æ®æºçš„ç»Ÿè®¡
            for source_id, items in results.items():
                logger.info(f"  {source_id}: {len(items)} ä¸ªæ–°æ–‡ç« ")
                
        except Exception as e:
            logger.error(f"å®šæ—¶çˆ¬å–å¤±è´¥: {e}")
            
    async def generate_summaries_job(self) -> None:
        """ç”Ÿæˆæ‘˜è¦çš„å®šæ—¶ä»»åŠ¡"""
        try:
            logger.info("å¼€å§‹å®šæ—¶ç”Ÿæˆæ‘˜è¦")
            start_time = datetime.now()
            
            await summary_generator.start_generation_cycle()
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logger.info(f"æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {duration:.2f} ç§’")
            
        except Exception as e:
            logger.error(f"å®šæ—¶ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            
    async def trigger_manual_crawl(self, source_id: str | None = None) -> Dict[str, Any]:
        """
        æ‰‹åŠ¨è§¦å‘çˆ¬å–ä»»åŠ¡
        
        Args:
            source_id: å¯é€‰çš„ç‰¹å®šæ•°æ®æºIDï¼Œä¸ºNoneæ—¶çˆ¬å–æ‰€æœ‰æº
            
        Returns:
            æ‰§è¡Œç»“æœç»Ÿè®¡
        """
        try:
            start_time = datetime.now()
            
            if source_id:
                logger.info(f"æ‰‹åŠ¨è§¦å‘çˆ¬å–ä»»åŠ¡ï¼Œæ•°æ®æº: {source_id}")
                new_items = await crawl_service.crawl_single_source(source_id)
                results = {source_id: new_items}
            else:
                logger.info("æ‰‹åŠ¨è§¦å‘çˆ¬å–ä»»åŠ¡ï¼Œæ‰€æœ‰æ•°æ®æº")
                results = await crawl_service.crawl_all_sources()
                
            total_new_items = sum(len(items) for items in results.values())
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            return {
                "success": True,
                "total_new_items": total_new_items,
                "duration_seconds": duration,
                "sources_crawled": len(results),
                "details": {
                    source: len(items) for source, items in results.items()
                }
            }
            
        except Exception as e:
            logger.error(f"æ‰‹åŠ¨è§¦å‘çˆ¬å–ä»»åŠ¡å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "total_new_items": 0,
                "duration_seconds": 0,
                "sources_crawled": 0,
                "details": {}
            }
            
    async def trigger_manual_summary_generation(self) -> Dict[str, Any]:
        """
        æ‰‹åŠ¨è§¦å‘æ‘˜è¦ç”Ÿæˆä»»åŠ¡
        
        Returns:
            æ‰§è¡Œç»“æœç»Ÿè®¡
        """
        try:
            logger.info("æ‰‹åŠ¨è§¦å‘æ‘˜è¦ç”Ÿæˆä»»åŠ¡")
            start_time = datetime.now()
            
            await summary_generator.start_generation_cycle()
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            return {
                "success": True,
                "duration_seconds": duration,
                "message": "Summary generation completed"
            }
            
        except Exception as e:
            logger.error(f"æ‰‹åŠ¨è§¦å‘æ‘˜è¦ç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "duration_seconds": 0,
                "message": "Summary generation failed"
            }
    
    def get_job_status(self) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡çŠ¶æ€ä¿¡æ¯"""
        jobs: list[dict[str, Any]] = []
        for job in self.scheduler.get_jobs():
            jobs.append({
                "id": job.id,
                "name": job.name,
                "next_run_time": job.next_run_time.isoformat() if job.next_run_time else None,
                "trigger": str(job.trigger),
            })
            
        return {
            "scheduler_running": self.scheduler.running,
            "jobs": jobs,
            "configuration": {
                "crawl_scheduler_enabled": self.settings.enable_crawl_scheduler,
                "summary_scheduler_enabled": self.settings.enable_summary_scheduler,
                "crawl_interval_minutes": self.settings.crawl_interval_minutes,
                "summary_concurrency": self.settings.summary_concurrency,
            }
        }


# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
task_scheduler = TaskScheduler()